# -*- coding: utf-8 -*-
"""CIS563: Term Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wulqq_BTW68lZIoHYEcaSf8oHZE1jecM

**Project Title: Movie Recommendation System using a Hybrid-Filtering Approach**
"""

#Importing Libraries
import pandas as pd
import numpy as np
#For content
from ast import literal_eval
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics.pairwise import linear_kernel, cosine_similarity
import matplotlib.pyplot as plt

"""**Data Pre-Processing**"""

#Reading the datasets
movies = pd.read_csv('/content/movies.csv')
ratings = pd.read_csv('/content/ratings.csv')
tags = pd.read_csv('/content/tags.csv')

#Shape of the datasets
print('movies: ', movies.shape)
print('ratings: ', ratings.shape)
print('tags: ', tags.shape)

movies.head(5)

ratings.head(5)

tags.head(5)

#Merging the movies and ratings datasets
data = pd.merge(ratings, movies, on='movieId' , how='left')
data.head()

data['genres'] = data['genres'].str.split('|')

data.head()

#Data Cleaning
data = data.drop('title', axis=1)
tags.drop('timestamp', axis=1, inplace=True)

#Splitting tags
tags['tag'] = tags['tag'].str.split('|')

#Here, we group the tags of a userID with its respective movieID
tags = tags.groupby(['userId','movieId'])['tag'].apply(lambda x: ','.join(x.astype(str))).reset_index()
tags.head(5)

#Merging the tags dataset with our dataset containing movies and ratings
data = pd.merge(data, tags, on=['userId','movieId'], how='left')

data.shape

data.head()

data['tag'] = data['tag'].apply(lambda d: d if isinstance(d, list) else [])
data['genres'] = data['genres'].apply(lambda d: d if isinstance(d, list) else [])

data.head()

"""**Train-Test Split**"""

from sklearn.model_selection import train_test_split
train_data, test_data = train_test_split(data, test_size=0.2, stratify=data.userId)

train_data = train_data.sort_values(['userId', 'movieId'])

test_data = test_data.sort_values(['userId','movieId'])

#Here, we save the datasets into csv files
 train_data.to_csv('training_data.csv', index = False)
 test_data.to_csv('testing_data.csv', index = False)

#Here, we pre-process the movie data
movies['genres'] = movies['genres'].str.split('|')
movies['genres'] = movies['genres'].apply(lambda d: d if isinstance(d, list) else [])
movies.head()

movies.to_csv('movies.csv', index = False)

"""**Content-Based Filtering**

In order to create a customer vector based on the content, this method investigates the genre and rating linked to the movie and the customer. This customer vector is used to produce the suggestions.
"""

# Movies data gets loaded
movies = pd.read_csv("movies.csv", converters={"genres": literal_eval})
movies.head()

# Training data gets loaded
ratings_train = pd.read_csv("training_data.csv", converters={"genres": literal_eval, "tag": literal_eval})

ratings_train.head()

#movies['tags'] = movies['tags'].fillna('')
#movies['description'] = movies['overview'] + movies['tagline']
#movies['description'] = movies['description'].fillna('')

# Unique genres
genre_unique = movies['genres'].explode().unique()

# Count of each genre
genre_count = ratings_train['genres'].explode().value_counts()

# Making a dicionary by assigning an index to a genre
genre_dict = {k: v for v, k in enumerate(genre_unique)}
genre_dict

movies['movie_vector'] = ""
for ind, row in movies.iterrows():
    genres = row.genres
    movie_vector = np.zeros(len(genre_dict))

    for g in genres:
        movie_vector[genre_dict[g]] = 1
    movies.at[ind, 'movie_vector'] = movie_vector

# Saving the final dataframe
movies.to_csv("movie_vector.csv")

movies.head()

#Personalising based on individual user
user_total = ratings_train['userId'].unique()
u_data = pd.DataFrame(columns=['userId', 'user_vector', 'avg_rating', 'num_movies_rated'])

for user_id in user_total:
    u_rating_data = ratings_train[(ratings_train['userId'] == user_id)]

    user_vector = np.zeros(len(genre_dict))
    count_vector = np.zeros(len(genre_dict))

    user_avg_rating = 0
    movies_rated_count = 0

    for _, row in u_rating_data.iterrows():
        user_avg_rating += row.rating
        movies_rated_count += 1
        genres = row.genres

        user_movie_vector = np.zeros(len(genre_dict))

        for g in genres:
            user_movie_vector[genre_dict[g]] = 1
            count_vector[genre_dict[g]] += 1

        user_vector += user_movie_vector*row.rating

    count_vector = np.where(count_vector==0, 1, count_vector)
    user_vector = np.divide(user_vector, count_vector)
    user_avg_rating /= movies_rated_count
    row_df = pd.DataFrame([[user_id, user_vector, user_avg_rating, movies_rated_count]],
                          columns=['userId', 'user_vector', 'avg_rating', 'num_movies_rated'])
    u_data = pd.concat([u_data, row_df], ignore_index=True)

#Saving user data into a dataframe
u_data.to_csv("user_info.csv")

u_data.head()

ratings_test = pd.read_csv("testing_data.csv", converters={"genres": literal_eval, "tag": literal_eval})
ratings_test.head()

ratings_test.iloc[0]

u_rating_data[u_rating_data['movieId']==1]

genres = u_rating_data[u_rating_data['movieId']==1].genres.values[0]
vector = np.zeros(len(genre_dict))
for g in genres:
    vector[genre_dict[g]] = 1
print(vector)

u_data[u_data['userId']==1].user_vector[0]

x = vector*u_data[u_data['userId']==1].user_vector[0]
np.nanmean(np.where(x!=0,x,np.nan))

contentbased_predictions = pd.DataFrame(columns=['userId', 'movieId', 'user_vector', 'movie_vector', 'og_rating', 'pred_rating'])
for ind, row in ratings_test.iterrows():
    userId = row['userId']
    movieId = row['movieId']
    og_rating = row['rating']

    try:
        user_vector = u_data[u_data['userId'] == int(userId)].user_vector.values[0]
        movie_vector = movies[movies['movieId'] == int(movieId)].movie_vector.values[0]

        predicted_rating = user_vector*movie_vector

        if predicted_rating.any():
            predicted_rating = np.nanmean(np.where(predicted_rating!=0, predicted_rating, np.nan))

        else:
            predicted_rating = 0

        row_df = pd.DataFrame([[userId, movieId, user_vector, movie_vector, og_rating, predicted_rating]],
                    columns=['userId', 'movieId', 'user_vector', 'movie_vector', 'og_rating', 'pred_rating'])
        contentbased_predictions = pd.concat([contentbased_predictions, row_df], ignore_index=True)
    except:
        print("User not found: ", userId)

#Calculating RMSE
rmse = ((contentbased_predictions.og_rating - contentbased_predictions.pred_rating) ** 2).mean() ** .5
print(rmse)

#Calculating MAE
mae = (((contentbased_predictions.og_rating - contentbased_predictions.pred_rating) ** 2) ** .5).mean()
mae

"""**KNN Analysis (CF)**

Here, I have provided a contrast between user-user based collaborative filtering with item-item based collaborative filtering.
"""

!pip install surprise

from surprise import SVD, BaselineOnly, SVDpp
from surprise import Dataset
from surprise.prediction_algorithms import KNNBaseline
from surprise import accuracy
from surprise.model_selection import train_test_split

# Here, I loaded the Movielens-1M dataset directly from the website to have more data
data = Dataset.load_builtin('ml-1m')

#Train-Test Split
trainset, testset = train_test_split(data, test_size=.20)

#Here, a function for the Collaborative Filtering Algorithm has been defined.
def recommendation(CFalgo, trainset, testset):
  CFalgo.fit(trainset)
  predictions = CFalgo.test(testset)
  #RMSE
  accuracy.rmse(predictions)
  #MAE
  accuracy.mae(predictions)
  return

# CF algorithm whihc takes a baseline rating
#Item-Item Based
similarity = {'name': 'cosine',
               'user_based': False
               }
CFalgo = KNNBaseline(sim_options=similarity)

CFalgo.fit(trainset)
predictions = CFalgo.test(testset)

accuracy.rmse(predictions)
accuracy.mae(predictions)
print("Done!")

#User-User based
similarity = {'name': 'cosine'}
CFalgo = KNNBaseline(similarity=similarity)
recommendation(CFalgo, trainset, testset)

"""**Generating predictions**"""

def traintest_conversion_for_surprise(training_dataframe, testing_dataframe):
    reader = Reader(rating_scale=(0, 5))
    trainset = Dataset.load_from_df(training_dataframe[['userId', 'movieId', 'rating']], reader)
    testset = Dataset.load_from_df(testing_dataframe[['userId', 'movieId', 'rating']], reader)
    trainset = trainset.construct_trainset(trainset.raw_ratings)
    testset = testset.construct_testset(testset.raw_ratings)
    return trainset, testset

def recommendation(CFalgo, trainset, testset):
    CFalgo.fit(trainset)

#Here, we make predictions on testing
    test_predictions = CFalgo.test(testset)
    test_rmse = accuracy.rmse(test_predictions)
    test_mae = accuracy.mae(test_predictions)

    return test_rmse, test_mae, test_predictions

file_path_train = 'training_data.csv'
file_path_test = 'testing_data.csv'
traindf = pd.read_csv(file_path_train)
testdf = pd.read_csv(file_path_test)
trainset, testset = traintest_conversion_for_surprise(traindf, testdf)

#1
CFalgo = BaselineOnly()
test_base_rmse, test_base_mae, test_base_pred = recommendation(CFalgo, trainset, testset)

#2
CFalgo = KNNBaseline(similarity=similarity)
test_knn_rmse, test_knn_mae, test_knn_pred = recommendation(CFalgo, trainset, testset)

# 3
CFalgo = SVD()
test_svd_rmse, test_svd_mae, test_svd_pred = recommendation(CFalgo, trainset, testset)

#4
CFalgo = SVDpp()
test_svdpp_rmse, test_svdpp_mae, test_svdpp_pred = recommendation(CFalgo, trainset, testset)

test_pred_df = pd.DataFrame(
    columns=['uid', 'iid', 'og_rating', 'svd_rating', 'knn_rating'])
test_svd_df = pd.DataFrame(
    columns=['uid', 'iid', 'og_rating', 'est_rating'])
test_svdpp_df = pd.DataFrame(
   columns=['uid', 'iid', 'og_rating', 'est_rating'])
test_knnb_df = pd.DataFrame(
    columns=['uid', 'iid', 'og_rating', 'est_rating'])

num_test = len(test_base_pred)
for i in range(num_test):
    svd = test_svd_pred[i]
    knn = test_knn_pred[i]
    svdpp = test_svdpp_pred[i]
    df = pd.DataFrame([[svd.uid, svd.iid, svd.r_ui, svd.est, knn.est]],
                      columns=['uid', 'iid', 'og_rating', 'svd_rating', 'knn_rating'])
    df_svd = pd.DataFrame([[svd.uid, svd.iid, svd.r_ui, svd.est]],
                          columns=['uid', 'iid', 'og_rating', 'est_rating'])
    df_svdpp = pd.DataFrame([[svd.uid, svd.iid, svd.r_ui, svdpp.est]],
                          columns=['uid', 'iid', 'og_rating', 'est_rating'])
    df_knnb = pd.DataFrame([[svd.uid, svd.iid, svd.r_ui, knn.est]],
                           columns=['uid', 'iid', 'og_rating', 'est_rating'])

    test_pred_df = pd.concat([df, test_pred_df], ignore_index=True)
    test_svd_df = pd.concat([df_svd, test_svd_df], ignore_index=True)
    test_svdpp_df = pd.concat([df_svdpp, test_svdpp_df], ignore_index=True)
    test_knnb_df = pd.concat([df_knnb, test_knnb_df], ignore_index=True)

#Making csvs
test_pred_df.to_csv('test_prediction_HP.csv')
test_svd_df.to_csv('test_predictions_svd.csv')
test_svdpp_df.to_csv('test_predictions_svdpp.csv')
test_knnb_df.to_csv('test_predictions_knnb.csv')

"""**Surprise model predictions**"""

# We will load the 1M dataset
data = Dataset.load_builtin('ml-1m')

#Train-Test Split
trainset, testset = train_test_split(data, test_size=.20)

def traintest_conversion_for_surprise(training_dataframe, testing_dataframe):
    reader = Reader(rating_scale=(0, 5))
    trainset = Dataset.load_from_df(training_dataframe[['userId', 'movieId', 'rating']], reader)
    testset = Dataset.load_from_df(testing_dataframe[['userId', 'movieId', 'rating']], reader)
    trainset = trainset.construct_trainset(trainset.raw_ratings)
    testset = testset.construct_testset(testset.raw_ratings)
    return trainset, testset

file_path_train = 'training_data.csv'
file_path_test = 'testing_data.csv'
traindf = pd.read_csv(file_path_train)
testdf = pd.read_csv(file_path_test)
trainset, testset = traintest_conversion_for_surprise(traindf, testdf)

def recommendation(CFalgo, trainset, testset):
  CFalgo.fit(trainset)
  test_predictions = CFalgo.test(testset)
  test_rmse = accuracy.rmse(test_predictions)
  test_mae = accuracy.mae(test_predictions)

  return test_rmse, test_mae, test_predictions

#Cross-Validation
#results = cross_validate(SVD(), data, measures=['RMSE', 'MAE'], cv=5, verbose=False)

# KNNBaseline

CFalgo = KNNBaseline()
test_knn_rmse, test_knn_mae, test_knn_pred = recommendation(CFalgo, trainset, testset)

# SVD

CFalgo = SVD()
test_svd_rmse, test_svd_mae, test_svd_pred  = recommendation(CFalgo, trainset, testset)

#SVDpp

algo = SVDpp()
test_svdpp_rmse, test_svdpp_mae, test_svdpp_pred = recommendation(algo, trainset, testset)

#BaselineOnly()

CFalgo = BaselineOnly()
test_base_rmse, test_base_mae, test_base_pred  = recommendation(CFalgo, trainset, testset)

test_pred_df = pd.DataFrame(columns= ['uid', 'iid', 'og_rating', 'svd_rating', 'knn_rating','svdpp_rating','baseline_rating'])

num_test = len(test_base_pred)
print(num_test)

for i in range(num_test):
  svd = test_svd_pred[i]
  knn = test_knn_pred[i]
  svdpp = test_svdpp_pred[i]
  baseline = test_base_pred[i]
  df = pd.DataFrame([[svd.uid, svd.iid, svd.r_ui, svd.est, knn.est, baseline.est]], columns=['uid', 'iid', 'og_rating', 'svd_rating', 'knn_rating', 'baseline_rating'])
  test_pred_df = pd.concat([df, test_pred_df], ignore_index=True)

test_pred_df

test_pred_df.to_csv('test_prediction.csv')

"""**Surprise Model Recommendation**"""

def traintest_conversion_for_surprise(training_dataframe, testing_dataframe):
    reader = Reader(rating_scale=(0, 5))
    trainset = Dataset.load_from_df(training_dataframe[['userId', 'movieId', 'rating']], reader)
    testset = Dataset.load_from_df(testing_dataframe[['userId', 'movieId', 'rating']], reader)
    trainset = trainset.construct_trainset(trainset.raw_ratings)
    testset = testset.construct_testset(testset.raw_ratings)
    return trainset, testset

file_path_train = 'training_data.csv'
file_path_test = 'testing_data.csv'
traindf = pd.read_csv(file_path_train)
testdf = pd.read_csv(file_path_test)
trainset, testset = traintest_conversion_for_surprise(traindf, testdf)

def get_top_n(predictions, n):
    # Mapping predcitions to each user
    top_n = defaultdict(list)
    original_ratings = defaultdict(list)

    for uid, iid, true_r, est, _ in predictions:
        top_n[uid].append((iid, est))
        original_ratings[uid].append((iid, true_r))

    # Sorting predictions and retriving the highest k
    for uid, user_ratings in top_n.items():
        user_ratings.sort(key=lambda x: x[1], reverse=True)
        top_n[uid] = user_ratings[:n]

    return top_n, original_ratings

#Calculating Precision, Recall and FMeasure
def precision_recall_at_k(predictions, k=5, threshold=3.5):

    # Mapping predictions to each and every user.
    user_est_true = defaultdict(list)
    for uid, _, true_r, est, _ in predictions:
        user_est_true[uid].append((est, true_r))

    precisions = dict()
    recalls = dict()
    for uid, user_ratings in user_est_true.items():

        user_ratings.sort(key=lambda x: x[0], reverse=True)
        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)
        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])
        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))
                              for (est, true_r) in user_ratings[:k])

        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1
        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1

    precision = (sum(prec for prec in precisions.values()) / len(precisions))
    recall = (sum(rec for rec in recalls.values()) / len(recalls))

    return precision, recall

def recommendation(CFalgo, trainset, testset):
  start_fit = time.time()
  CFalgo.fit(trainset)
  end_fit = time.time()
  fit_time = end_fit - start_fit

  start_test = time.time()
  test_predictions = CFalgo.test(testset)
  end_test = time.time()
  test_time = end_test - start_test

  test_rmse = accuracy.rmse(test_predictions)
  test_mae = accuracy.mae(test_predictions)

  top_n, org_ratings = get_top_n(test_predictions, 5)

  precision, recall = precision_recall_at_k(test_predictions)

  f_measure = (2*precision*recall)/(precision+recall)

  return (test_rmse, test_mae, fit_time, test_time, precision, recall, f_measure, test_predictions)

#Collaborative Filtering Model
similarity = {'name': 'cosine',
               'user_based': False
               }
algo = KNNBaseline(similarity=similarity)

results = recommendation(CFalgo,trainset,testset)
print(results[0])
print(results[1])
print(results[2])
print(results[3])
print(results[4])
print(results[5])
print(results[6])
print(results[7])

CFalgo = CoClustering(2,5,50)

test_rmse, test_mae, test_predictions, fit_time, test_time, precision, recall, f_measure = recommendation(CFalgo,trainset,testset)
print(test_rmse)
print(test_mae)
print(fit_time)
print(test_time)
print(precision)
print(recall)
print(f_measure)

surprise_df = pd.DataFrame(columns= ['Algorithm', 'test_rmse', 'test_mae', 'fit_time', 'test_time', 'Precision', 'Recall', 'F-measure'])

# Here, we iterate iver all the algorithms we have chosen
for algorithm in [SVD(), KNNBaseline(), BaselineOnly(), CoClustering()]:
    results = recommendation(algorithm,trainset,testset)

    name =str(algorithm).split(' ')[0].split('.')[-1]
    print("Algorithm:", name)
    df = pd.DataFrame([[name, results[0], results[1], results[2], results[3], results[4], results[5], results[6]]], columns= ['Algorithm', 'test_rmse', 'test_mae', 'fit_time', 'test_time', 'Precision', 'Recall', 'F-measure'])
    surprise_df = pd.concat([df, surprise_df], ignore_index=True)
surprise_df.sort_values(by='test_rmse', ascending=False)

surprise_df.sort_values(by='test_rmse')

surprise_df.sort_values(by='F-measure', ascending=False)

surprise_df.head()

surprise_df.to_csv('Surprise_results.csv')

surprise_df.sort_values(by='test_rmse')

surprise_df.sort_values(by='F-measure', ascending=False)

"""**Combined Model**"""

import pandas as pd
import numpy as np
import math

pred_data = pd.read_csv('test_prediction_HP.csv')
pred_data.head()

T = pred_data.shape[0]
print(T)

svd_wt = 0.05
knn_wt = 0.6
svdpp_wt = 0.4
baseline_wt = 0

rmse = ((pred_data.og_rating - pred_data.knn_rating) ** 2).mean() ** .5
print(rmse)
mae = (((pred_data.og_rating - pred_data.knn_rating) ** 2) ** .5).mean()
print(mae)

rmse = ((pred_data.og_rating - pred_data.svdpp_rating) ** 2).mean() ** .5
print(rmse)
mae = (((pred_data.og_rating - pred_data.svdpp_rating) ** 2) ** .5).mean()
print(mae)

sqr_sum = 0
abs_sum = 0

for ind, row in pred_data.iterrows():
  org_r = row['og_rating']
  pred_r = svd_wt*row['svd_rating'] + knn_wt*row['knn_rating']
  diff = np.abs(org_r - pred_r)
  # print(diff)
  abs_sum += diff
  sqr_sum += diff**2

rmse = np.sqrt(sqr_sum/T)
print("RMSE", rmse)
mae = abs_sum/T
print("MAE", mae)

"""**Hyperparameter Tuning + Matrix Factorization**"""

import time
import pandas as pd

def traintest_conversion_for_surprise(training_dataframe, testing_dataframe):
    reader = Reader(rating_scale=(0, 5))
    trainset = Dataset.load_from_df(training_dataframe[['userId', 'movieId', 'rating']], reader)
    testset = Dataset.load_from_df(testing_dataframe[['userId', 'movieId', 'rating']], reader)
    trainset = trainset.construct_trainset(trainset.raw_ratings)
    testset = testset.construct_testset(testset.raw_ratings)
    return trainset, testset

file_path_train = 'training_data.csv'
file_path_test = 'testing_data.csv'
traindf = pd.read_csv(file_path_train)
testdf = pd.read_csv(file_path_test)
trainset, testset = traintest_conversion_for_surprise(traindf, testdf)

def recommendation(CFalgo, trainset, testset):
  start_fit = time.time()
  CFalgo.fit(trainset)
  end_fit = time.time()
  fit_time = end_fit - start_fit

  start_test = time.time()
  test_predictions = CFalgo.test(testset)
  end_test = time.time()
  test_time = end_test - start_test

  test_rmse = accuracy.rmse(test_predictions)
  test_mae = accuracy.mae(test_predictions)

  return test_rmse, test_mae, test_predictions, fit_time, test_time

CFalgo = BaselineOnly()

test_rmse, test_mae, test_predictions, fit_time, test_time = recommendation(CFalgo,trainset,testset)
print(fit_time)
print(test_time)

# Probabilistic Matrix Factorization

CFalgo = SVD()

CFalgo.fit(trainset)
predictions = CFalgo.test(testset)

accuracy.rmse(predictions)
accuracy.mae(predictions)
print("Done!")

# SVDpp (an extension of SVD which takes implicit ratings)

CFalgo = SVDpp()

CFalgo.fit(trainset)
predictions = CFalgo.test(testset)

accuracy.rmse(predictions)
accuracy.mae(predictions)
print("Done!")

# CF Algorithm
CFalgo = KNNBaseline()

CFalgo.fit(trainset)
predictions = CFalgo.test(testset)

accuracy.rmse(predictions)
accuracy.mae(predictions)
print("Done!")

from surprise import NormalPredictor
from surprise.model_selection import GridSearchCV

#Implementing GridSearch for Latent Factor
parameter_grid = {'n_factors':[25,50,100], 'n_epochs': [5, 10, 20], 'lr_all': [0.01, 0.02],
              'reg_all': [0.01,0.02]}
gs = GridSearchCV(SVD, parameter_grid, measures=['rmse', 'mae'], cv=5)
gs.fit(data)

# Best RMSE
print(gs.best_score['rmse'])
# Parameters which gave best RMSE
print(gs.best_params['rmse'])

# best RMSE score
print(gs.best_score['mae'])

"""**Cold-Start Analysis**"""

def traintest_conversion_for_surprise(training_dataframe, testing_dataframe):
    reader = Reader(rating_scale=(0, 5))
    trainset = Dataset.load_from_df(training_dataframe[['userId', 'movieId', 'rating']], reader)
    testset = Dataset.load_from_df(testing_dataframe[['userId', 'movieId', 'rating']], reader)
    trainset = trainset.construct_trainset(trainset.raw_ratings)
    testset = testset.construct_testset(testset.raw_ratings)
    return trainset, testset

file_path_train = 'training_data.csv'
file_path_test = 'testing_data.csv'
traindf = pd.read_csv(file_path_train)
testdf = pd.read_csv(file_path_test)
trainset, testset = traintest_conversion_for_surprise(traindf, testdf)

traindf.head()

CFalgo_svd = SVD()
CFalgo_svdpp = SVDpp()
CFalgo_knn = KNNBaseline()

CFalgo_svd.fit(trainset)
predictions_svd = CFalgo_svd.test(testset)

CFalgo_svdpp.fit(trainset)
predictions_svdpp = CFalgo_svdpp.test(testset)

CFalgo_knn.fit(trainset)
predictions_knn = CFalgo_knn.test(testset)

dump.dump('./dump_SVD', predictions_svd, CFalgo_svd)
dump.dump('./dump_SVDpp', predictions_svdpp, CFalgo_svdpp)
dump.dump('./dump_KNN', predictions_knn, CFalgo_knn)

df_svd = pd.DataFrame(predictions_svd, columns=['uid', 'iid', 'rui', 'est', 'details'])
df_svdpp = pd.DataFrame(predictions_svdpp, columns=['uid', 'iid', 'rui', 'est', 'details'])
df_knn = pd.DataFrame(predictions_knn, columns=['uid', 'iid', 'rui', 'est', 'details'])

similarity = {'name': 'cosine',
               'user_based': False
               }
CFalgo_knnbaseline = KNNBaseline(similarity=similarity)
CFalgo_knnbaseline.fit(trainset)
predictions_knnbaseline = CFalgo_knnbaseline.test(testset)

df_knnbaseline = pd.DataFrame(predictions_knnbaseline, columns=['uid', 'iid', 'rui', 'est', 'details'])
df_knnbaseline['err'] = abs(df_knnbaseline.est - df_knnbaseline.rui)
df_knnbaseline['sqr_err'] = (df_knnbaseline.est - df_knnbaseline.rui)**2

df_svd['err'] = abs(df_svd.est - df_svd.rui)
df_svdpp['err'] = abs(df_svdpp.est - df_svdpp.rui)
df_knn['err'] = abs(df_knn.est - df_knn.rui)

df_svd['sqr_err'] = (df_svd.est - df_svd.rui)**2
df_svdpp['sqr_err'] = (df_svdpp.est - df_svdpp.rui)**2
df_knn['sqr_err'] = (df_knn.est - df_knn.rui)**2

CFalgo_baselineonly = BaselineOnly()
CFalgo_baselineonly.fit(trainset)
predictions_baselineonly = CFalgo_baselineonly.test(testset)

df_baselineonly = pd.DataFrame(predictions_baselineonly, columns=['uid', 'iid', 'rui', 'est', 'details'])
df_baselineonly['err'] = abs(df_baselineonly.est - df_baselineonly.rui)
df_baselineonly['sqr_err'] = (df_baselineonly.est - df_baselineonly.rui)**2
#df_baselineonly['Iu'] = df_baselineonly.uid.apply(get_Iu)

similarity = {'name': 'cosine',
               'user_based': True  # compute  similarities between items
               }
CFalgo_knnbaseline_user = KNNBaseline(similarity=similarity)
CFalgo_knnbaseline_user.fit(trainset)
predictions_knnbaseline_user = CFalgo_knnbaseline_user.test(testset)

df_knn_user = pd.DataFrame(predictions_knnbaseline_user, columns=['uid', 'iid', 'rui', 'est', 'details'])
df_knn_user['err'] = abs(df_knn_user.est - df_knn_user.rui)
df_knn_user['sqr_err'] = (df_knn_user.est - df_knn_user.rui)**2
#df_knn_user['Iu'] = df_knn_user.uid.apply(get_Iu)

df_svd.head()

def get_Iu(uid):
    try:
        return traindf[traindf['userId'] == uid].shape[0]
    except ValueError:  # user was not part of the trainset
        return 0

df_knn['Iu'] = df_knn.uid.apply(get_Iu)
df_svd['Iu'] = df_svd.uid.apply(get_Iu)
df_svdpp['Iu'] = df_svdpp.uid.apply(get_Iu)
df_knnbaseline['Iu'] = df_knnbaseline.uid.apply(get_Iu)

print("--------------------------MAE-----------------------")
print("KNN Basic                 ",df_knn[df_knn.Iu < 18].err.mean())
print("SVD                       ", df_svd[df_svd.Iu < 18].err.mean())
print("SVDpp                     ",  df_svdpp[df_svdpp.Iu < 18].err.mean())
print("KNN Baseline (item-item)  ", df_knnbaseline[df_knnbaseline.Iu < 18].err.mean())
#print("BaselineOnly              ",df_baselineonly[df_baselineonly.Iu < 18].err.mean() )
#print("KNN Baseline (user-user)  ",df_knn_user[df_knn_user.Iu < 18].err.mean() )

print("--------------------------RMSE-----------------------")
print("KNN Basic                ",df_knn[df_knn.Iu < 18].sqr_err.mean()** .5)
print("SVD                      ", df_svd[df_svd.Iu < 18].sqr_err.mean()** .5)
print("SVDpp                    ",  df_svdpp[df_svdpp.Iu < 18].sqr_err.mean()** .5)
print("KNN Baseline (item-item) ", df_knnbaseline[df_knnbaseline.Iu < 18].sqr_err.mean()** .5)
#print("BaselineOnly             ",df_baselineonly[df_baselineonly.Iu < 18].sqr_err.mean()** .5 )
#print("KNN Baseline (user-user) ",df_knn_user[df_knn_user.Iu < 18].sqr_err.mean()** .5)

print("--------------------------MAE-----------------------")
print("KNN Basic                 ",df_knn[df_knn.Iu > 1000].err.mean())
print("SVD                       ", df_svd[df_svd.Iu > 1000].err.mean())
print("SVDpp                     ",  df_svdpp[df_svdpp.Iu > 1000].err.mean())
print("KNN Baseline (item-item)  ", df_knnbaseline[df_knnbaseline.Iu > 1000].err.mean())
#print("BaselineOnly              ",df_baselineonly[df_baselineonly.Iu > 1000].err.mean() )
#print("KNN Baseline (user-user)  ",df_knn_user[df_knn_user.Iu > 1000].err.mean() )

iid_df = traindf.groupby(['userId'],as_index=False).movieId.count()
iid_df.movieId.max()

"""**Popularity**"""

movies = pd.read_csv("/content/tmdb_5000_movies.csv")

movies.sort_values(by='popularity', ascending=False)

genres = {'Adventure': 0,
 'Animation': 1,
 'Children': 2,
 'Comedy': 3,
 'Fantasy': 4,
 'Romance': 5,
 'Drama': 6,
 'Action': 7,
 'Crime': 8,
 'Thriller': 9,
 'Horror': 10,
 'Mystery': 11,
 'Sci-Fi': 12,
 'War': 13,
 'Musical': 14,
 'Documentary': 15,
 'IMAX': 16,
 'Western': 17,
 'Film-Noir': 18,
 '(no genres listed)': 19}

def genre_based_on_popularity(genre):
    mask = movies.genres.apply(lambda x: genre in x)
    filtered_movie = movies[mask]
    filtered_movie = filtered_movie.sort_values(by='popularity', ascending=False)
    return filtered_movie

genre_based_on_popularity('Animation')[['title', 'popularity']].head(25)

genre_based_on_popularity('Romance')[['title', 'popularity']].head(25)

genre_based_on_popularity('Action')[['title', 'popularity']].head(25)

vote_counts = movies[movies['vote_count'].notnull()]['vote_count'].astype('int') #V

vote_averages = movies[movies['vote_average'].notnull()]['vote_average'].astype('int') #R

C = vote_averages.mean()
C

m = vote_counts.quantile(0.95)
m

def weighted_rating(x):
    v = x['vote_count']
    R = x['vote_average']
    return (v/(v+m) * R) + (m/(m+v) * C)

movies['wr'] = movies.apply(weighted_rating, axis=1)

movies.head()

def genre_based_popularity_PT(genre):
    mask = movies.genres.apply(lambda x: genre in x)
    filtered_movie = movies[mask]
    filtered_movie = filtered_movie.sort_values(by='wr', ascending=False)
    return filtered_movie

genre_based_popularity_PT('Animation')[['title', 'wr', 'popularity']].head(10)

genre_based_popularity_PT('Action')[['title', 'wr', 'popularity']].head(25)

genre_based_popularity_PT('Romance')[['title', 'wr', 'popularity']].head(25)

"""**Hybrid Model**"""

def traintest_conversion_for_surprise(training_dataframe, testing_dataframe):
    reader = Reader(rating_scale=(0, 5))
    trainset = Dataset.load_from_df(training_dataframe[['userId', 'movieId', 'rating']], reader)
    testset = Dataset.load_from_df(testing_dataframe[['userId', 'movieId', 'rating']], reader)
    trainset = trainset.construct_trainset(trainset.raw_ratings)
    testset = testset.construct_testset(testset.raw_ratings)
    return trainset, testset

file_path_train = 'training_data.csv'
file_path_test = 'testing_data.csv'
traindf = pd.read_csv(file_path_train)
testdf = pd.read_csv(file_path_test)
trainset, testset = traintest_conversion_for_surprise(traindf, testdf)

testdf.head()

similarity = {'name': 'cosine',
               'user_based': False
               }
knnbaseline_algo = KNNBaseline(similarity=similarity)

knnbaseline_algo.fit(trainset)
knnbaseline_predictions = knnbaseline_algo.test(testset)

file_name = 'KnnBaseline_model'
dump.dump(file_name, algo=knnbaseline_predictions)

accuracy.rmse(knnbaseline_predictions)
accuracy.mae(knnbaseline_predictions)
print("Done!")

svd_algo = SVD()

svd_algo.fit(trainset)
svd_predictions = svd_algo.test(testset)

file_name = 'svd_model'
dump.dump(file_name, algo=svd_algo)

accuracy.rmse(svd_predictions)
accuracy.mae(svd_predictions)
print("Done!")

svdpp_algo = SVDpp()

svdpp_algo.fit(trainset)
svdpp_predictions = svdpp_algo.test(testset)

file_name = 'svd_model'
dump.dump(file_name, algo=svdpp_algo)

accuracy.rmse(svdpp_predictions)
accuracy.mae(svdpp_predictions)
print("Done!")

knn_baseline = dump.load('KnnBaseline_model')
svdpp = dump.load('svd_model')

# Users in testing data represented as a list
user_list = testdf['userId'].unique()

test_movies = testdf[testdf['userId'] == 60]
test_movies.head()

movies = pd.read_csv("movie_vector.csv")

genre_to_idx = {'Adventure': 0,
 'Animation': 1,
 'Children': 2,
 'Comedy': 3,
 'Fantasy': 4,
 'Romance': 5,
 'Drama': 6,
 'Action': 7,
 'Crime': 8,
 'Thriller': 9,
 'Horror': 10,
 'Mystery': 11,
 'Sci-Fi': 12,
 'War': 13,
 'Musical': 14,
 'Documentary': 15,
 'IMAX': 16,
 'Western': 17,
 'Film-Noir': 18,
 '(no genres listed)': 19}

idx_to_genre = {0: 'Adventure',
 1: 'Animation',
 2: 'Children',
 3: 'Comedy',
 4: 'Fantasy',
 5: 'Romance',
 6: 'Drama',
 7: 'Action',
 8: 'Crime',
 9: 'Thriller',
 10: 'Horror',
 11: 'Mystery',
 12: 'Sci-Fi',
 13: 'War',
 14: 'Musical',
 15: 'Documentary',
 16: 'IMAX',
 17: 'Western',
 18: 'Film-Noir',
 19: '(no genres listed)'}

movies.head()

tf_new = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), min_df=0, stop_words='english')
tfidf_matrix_new = tf_new.fit_transform(movies['genres'])

cosine_sim_new = linear_kernel(tfidf_matrix_new, tfidf_matrix_new)

movies = movies.reset_index()
titles = movies['title']
indices = pd.Series(movies.index, index=movies['title'])
indices.head(2)

def get_recommendations_new(title):
    idx = indices[title]
    if type(idx) != np.int64:
        if len(idx)>1:
            print("ALERT: Multiple values")
            idx = idx[0]
    sim_scores = list(enumerate(cosine_sim_new[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:11]
    movie_indices = [i[0] for i in sim_scores]
    return movies['movieId'].iloc[movie_indices]

def genre_based_on_popularity(genre):
    mask = movies.genres.apply(lambda x: genre in x)
    filtered_movie = movies[mask]
    filtered_movie = filtered_movie.sort_values(by='popularity', ascending=False)
    return filtered_movie['movieId'].head(10).values.tolist()

user_info = pd.read_csv('user_info.csv')

user_info['user_vector'] = user_info['user_vector'].apply(lambda x: x.replace('[', ' ').replace(']', ' ').strip().split())
user_info['user_vector'] = user_info['user_vector'].apply(lambda x: np.asarray(x).astype(float))

def user_top_genre(userId):
    user_vec = user_info['user_vector'][user_info['userId'] == userId].values[0].copy()
    print("User Vector: ", user_vec)
    top_genre_indices = np.flip(np.argsort(user_vec))
    genre_list = []
    for i in top_genre_indices[:3]:
        genre_list.append(idx_to_genre[i])
    return genre_list

user_top_genre(1)

knn_baseline = dump.load('KnnBaseline_model')
svdpp = dump.load('svd_model')

user_list = testdf['userId'].unique()

test_movies = testdf[testdf['userId'] == 60]
test_movies.head()

#Hybrid Model

def hybrid(userId):
    user_movies = testdf[testdf['userId'] == userId]
    user_movies['est'] = user_movies['movieId'].apply(lambda x: 0.6*knnbaseline_algo.predict(userId,x).est + 0.4*svdpp_algo.predict(userId, x).est)
    user_movies = user_movies.sort_values(by ='est', ascending=False).head(4)
    user_movies['Model'] = 'SVD + CF'

    recommend_list = user_movies[['movieId', 'est', 'Model']]
    print(recommend_list.head())

    movie_list = recommend_list['movieId'].values.tolist()
    print(movie_list)
    sim_movies_list = []
    for movie_id in movie_list:
        movie_title = movies['title'][movies['movieId'] == movie_id].values[0]
        sim_movies = get_recommendations_new(movie_title)
        sim_movies_list.extend(sim_movies)


    # Ratings for the popular movies are computed
    for movie_id in sim_movies_list:
        pred_rating = 0.6*knnbaseline_algo.predict(userId, movie_id).est + 0.4*svdpp_algo.predict(userId, movie_id).est
        row_df = pd.DataFrame([[movie_id, pred_rating, 'Movie similarity']], columns=['movieId', 'est','Model'])
        recommend_list = pd.concat([recommend_list, row_df], ignore_index=True)

    return recommend_list

traindf[traindf['userId'] == 524].sort_values(by = 'rating', ascending = False)

testdf[testdf['userId'] == 574]

movie_ids = hybrid(1)

def get_title(x):
    mid = x['movieId']
    return movies['title'][movies['movieId'] == mid].values

def get_genre(x):
    mid = x['movieId']
    return movies['genres'][movies['movieId'] == mid].values

movie_ids['title'] = movie_ids.apply(get_title, axis=1)
movie_ids['genre'] = movie_ids.apply(get_genre, axis=1)

movie_ids.sort_values(by='est', ascending = False).head(10)

